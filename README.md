# Question-controlled Text-aware Image Captioning (ACM MM2021)

By Anwen Hu, Shizhe Chen, Qin Jin

# Dataset
## Get QcTextCap Datasets
Download ControlTextCaps and ControlVizWiz from baidu disk (https://pan.baidu.com/s/1gw5l6eFFGO2OFfWt9ZtFAg, pwdï¼šykyi).
Put corresponding imdb directories under `data/$dataset_name`.
Raw images can be downloaded from official sites of [TextCaps](https://textvqa.org/textcaps/dataset/) and [VizWiz-Captions](https://vizwiz.org/tasks-and-datasets/image-captioning/).
 
 
## Dataset Introduction
The text-aware captions, questions and basic information of images are all stored in .npy files. 
Each sample in .npy file represent a tuple `<image, text-aware caption, automatic initial caption, pseudo initial caption, questions>`.
The file can be read
with following codes:

```
import numpy as np
data_path = ...
data = np.load(data_path, allow_pickle=True)
for sample in data:
    print(sample)
 ```
 Each `sample` is a dictionary where `caption_str` refers to the `text-aware caption`,`simple_caption_str` refers to the `automatic initial caption`, 
 `model_simple_caption_str` refers to the `pseudo initial caption` generated by a in-domain AoANet, 
 `auto_question_str` refers to `questions` joined with the token \<ANS>. 
 Besides, each `sample` also stores the object and ocr bounding boxes in `obj_normalized_boxes` and `ocr_normalized_boxes`.
 Extract object-level features of these bounding boxes by [bottom-up-attention](https://github.com/MILVLG/bottom-up-attention.pytorch) (for the config file, we choose `configs/bua-caffe/extract-bua-caffe-r152.yaml`) and put these features
 under  `data/$dataset_name/bua_feats/obj` and `data/$dataset_name/bua_feats/ocr`.
 
 
 # Experiments
 
 ## environment
 This project is revised based on M4C-Captioner, so please prepare environments as [pythia](https://github.com/facebookresearch/mmf/tree/project/m4c).
 
 ## checkpoints download
Checkpoints of M4CC, GQAM w/o GE, GQAM can be download from baidu disk (https://pan.baidu.com/s/1g8GzWAu0gVRlxGiphgDmsg, pwd:w4a6).
  
 
 ## train from scratch
 To train GQAM from scratch (e.g. GQAM with rand training strategy on ControlTextCaps), run the shell script as follows:
 ```
CUDA_VISIBLE_DEVICES=0 sh run_train_controltextcaps.sh
```
 
 ## test 
 To test a model (e.g. GQAM trained with rand strategy on ControlTextCaps), run the shell script to get captions as follows:
 
  ```
CUDA_VISIBLE_DEVICES=0 sh run_test_controltextcaps.sh
```
Copy the path of prediction file to `eval_QcTextCap.sh`, run this shell script to calculate captioning metrics.

 
 #Citation

If you find this code useful for your research, please consider citing:
```bibtex
@inproceedings{DBLP:conf/mm/HuCJ21,
  author    = {Anwen Hu and
               Shizhe Chen and
               Qin Jin},
  title     = {Question-controlled Text-aware Image Captioning},
  booktitle = {{ACM} Multimedia},
  pages     = {3097--3105},
  publisher = {{ACM}},
  year      = {2021}
}
```
 
 
 
 
 
