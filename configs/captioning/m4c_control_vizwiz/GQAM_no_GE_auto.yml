includes:
- common/defaults/configs/datasets/captioning/m4c_control_textcaps.yml
# Use soft copy
dataset_attributes:
  m4c_control_textcaps:
    obj_features_max_len: 100
    ocr_features_max_len: 50
    image_features:
      train:
      - ControlVizWiz/bua_feats/obj,ControlVizWiz/bua_feats/ocr
      val:
      - ControlVizWiz/bua_feats/obj,ControlVizWiz/bua_feats/ocr
      test:
      - ControlVizWiz/bua_feats/obj,ControlVizWiz/bua_feats/ocr
    imdb_files:
      train:
      - ControlVizWiz/imdb/imdb_azure_train_v2a.npy
      val:
      - ControlVizWiz/imdb/imdb_azure_val_v2a.npy
      test:
      - ControlVizWiz/imdb/imdb_azure_test_v2a.npy
    processors:
      text_processor: # process simple caption and auto question separately
        type: bert_tokenizer_control
        params:
          use_model_simple_cap: true
          simul_user_question: false # anwen hu 2021/3/19 whether to use simulated questions
          simul_cleanuser_question: false # anwenhu 2021/3/25
          simul_user_question_num: 2 # anwen hu 2021/3/19 the number of simulated questions
          use_model_and_auto_simpel_cap: false
          model_simple_cap_prob: 0.5 # anwen hu 2021/3/10 only used when use_model_and_auto_simpel_cap is true
          use_human_anno: false # use human simple cap and human questions (only true when inference on annotest)
          max_length: 20 # max length for simple caption or concatenated question
          max_question_num: 5 # max question num
          single_que_max_length: 10 # only used for separated questions
          max_ocrtoken_length: 7 # max word num in one ocr token
      answer_processor: # process gt caption
        type: m4c_azure_caption # old: m4c_caption
        params:
          vocab_file: ControlVizWiz/vizwiz_vocab_m4c_threshold_5.txt # vizwiz_vocab_m4c_threshold_5.txt #
          preprocessor:
            type: simple_word
            params: {}
          context_preprocessor:
            type: simple_word
            params: {}
          max_length: 50 # max ocr dictionary length
          max_copy_steps: 30 # max seq_length
          num_answers: 1 # for each data item, there is one image and one caption
          ignore_unk_target: true # anwen hu 2021/3/12 whether to ignore the loss of unk
      copy_processor:
        type: copy
        params:
          max_length: 100
      phoc_processor:
        type: phoc
        params:
          max_length: 50
model_attributes:
  hie_control_captioner:
    lr_scale_frcn: 0.1
    lr_scale_text_bert: 0.1
    lr_scale_mmt: 1.0  # no scaling
    text_bert_init_from_bert_base: true
    text_bert:
      num_hidden_layers: 3
    obj:
      mmt_in_dim: 2048
      dropout_prob: 0.1
    ocr:
      mmt_in_dim: 3002  # 300 (FastText) + 604 (PHOC) + 2048 (Faster R-CNN) + 50 (all zeros; legacy)
      dropout_prob: 0.1
    vqammt:
      hidden_size: 768
      num_hidden_layers: 2
      drop_obj: false # anwen hu 2021/7/23
      drop_ocr: false # anwen hu 2021/7/23
      init_use_bbox_att_purevision: false # anwen hu 2021/03/03
      objocr_pre_encoding_layers: 2 # anwen hu 2021/03/03 only used when init_use_bbox_att_purevision_vqammt is true
      sep_question_emb: false # anwenhu 2021/3/23 whether to emb each question to a global vector
      only_attend_vision: true
    capmmt:
      hidden_size: 768
      num_hidden_layers: 4
      drop_auto_question: false # anwen hu 2021/1/20 # priority is bigger than use_raw_que and use_vision_que
      drop_simple_cap: false # anwen hu 2021/7/23
      mid_use_bbox_att: false # anwen hu 2020/10/7
      init_use_bbox_att: false # anwen hu 2020/10/13
      use_vqa_obj: true # anwen hu 2021/2/25 # if vqammt.drop_obj, obj is also not used in capmmt
      use_vqa_ocr: true # anwen hu 2021/3/12 # if vqammt.drop_ocr, ocr is also not used in capmmt
      use_raw_que: true # anwen hu 2021/3/12
      use_vision_que: true # anwen hu 2021/3/13
      que_vision_txt_fuse_type: add # anwenhu 2021/3/15 cat; add
      avoid_repeat: true

    classifier:
      type: linear
      ocr_max_num: 50
      ocr_ptr_net:
        hidden_size: 768
        query_key_size: 768
      params: {}
    model_data_dir: ../data
    metrics:
    - type: textcaps_bleu4
    # - type: controlvqa_accuracy # anwen hu 2021/2/24
    losses:
    - type: m4c_decoding_bce_with_mask # anwen hu 2020/11/15; m4c original: m4c_decoding_bce_with_mask
    # - type: control_ans_ce_with_mask # anwen hu 2021/2/24
    remove_unk_in_pred: true
optimizer_attributes:
  params:
    eps: 1.0e-08
    lr: 1e-4
    weight_decay: 0
  type: Adam
training_parameters:
    clip_norm_mode: all
    clip_gradients: true
    max_grad_l2_norm: 0.25
    lr_scheduler: true
    lr_steps:
    - 8000
    - 11000
    lr_ratio: 0.1
    use_warmup: true
    warmup_factor: 0.2
    warmup_iterations: 400 # 400 for 8000 iterations
    # for controlvizwiz (25000 train), 8000*50 = 16 epoch
    max_iterations: 10000
    batch_size: 50 # 128
    log_interval: 50 # 100 for 1600 iterations, 50 for 8000 iterations
    snapshot_interval: 500 #  1000 for 16000 iterations, 500 for 8000 iterations
    num_workers: 4 # default 8 (202.112.113.77 shm 64G)
    task_size_proportional_sampling: true
    monitored_metric: m4c_control_textcaps/textcaps_bleu4
    metric_minimize: false
    sc_learning: false # anwen hu 2020/11/11 whether to use self-critical learning
